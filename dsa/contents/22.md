### 分而治之 divide and conquer 概述

- 分而治之，就像是团队合作，将一项工作分解成不同的成员去处理

<div align="center">
    <img width="600" src="./screenshot/124.jpg">
    <br />
    <div style="text-align:center">备注：图片托管于github，请确保网络的可访问性</div>
    <br />
</div>

- 一般情况，我们面临的规模比较大，用n来表示，我们会把它分为若干个任务来同步进行处理
- 一般而言，两个其实就可以了，相较于1个来说有本质的提升
- 分而治之作为解决问题策略的一种，有特殊的条件
    * 分得的两个子问题subproblem的规模要相当，差不太多
    * 两个子问题的规模加在一起不能超过之前的规模 (因为是递归分下去的，一次膨胀一点，很快这个问题本身就会爆炸)
    * 两个子问题彼此不要相关 (有时候往往做不到，我们所研究的大部分都是能做到的，如果两部分耦合性很强，就需要借助动态规划来实现)
- 我们将一个问题分解为两个问题来计算，计算完成之后，就会得到子任务的解，这些解不是最终问题的解，还需要merge起来，最后才能得到原始问题的答案
- 我们大部分实现减而治之的方式就是递归，但是递归都会存在一些问题
    * 系统为了实现递归要维护很多栈的资源，当子任务不断往下递归下去的时候占用资源就会非常多，以至于"爆栈"
    * 使用递归的解决方案不能扩展的很大，总是局限在很小的范围来解决
    * 几乎我们没有其他好的实现办法，只能借助递归
- 在递归算法分析的时候，往往直接画出图来，就能知道哪个函数调用的谁，以至很方便的得到复杂度的估计

### 主定理 Master Theorem

- 这时候评估递归算法的复杂度，我们需要以抽象对抽象，用方程或递推式来解决，如下

<div align="center">
    <img width="600" src="./screenshot/125.jpg">
    <br />
    <div style="text-align:center">备注：图片托管于github，请确保网络的可访问性</div>
    <br />
</div>

- 我们经常写出如上的递推式，如：$T(n) = a \cdot T(n/b) + O(f(n))$
    * 分而治之，将规模为n的问题，分为了a个子问题，每个子问题的规模是 $n/b$
    * O(f(n))表示divide任务的时间加上merge任务的时间，和n相关的函数f(n)
    * 大多数递推式通过挪项，配平等都可以算出来
    * 递归一般有很多诀窍可以概括为 Master Theorem **主定理**
        * 它告诉我们这一类的递推式 T(n)，以后只需要看这三个参数即可：a,b,f(n)
        * a: 分出的每一个子任务的数目，一般是2
        * b: $n/b$ 一般是subproblem 的 problem size， b 一般来说是2
        * f(n): 分与和需要的时间
        * a,b,f(n)总共三种大的情况，如上图所示
        * 一般，我们将$a \cdot T(n/b)$构造出一个函数：$log_b a$
          * 这里的b暗示的是取对数的基底: base
        * 然后比较$n^{log_b a}$与$O(f(n))$之间的大小关系
    * 上图表格第一排中$o(n^{log_b a})$，这里的o叫做Small-O，注意Big-O在英文中准确的叫法是Capital-O, Small-O和Big-O在本质上是一样的
        * Big-O通常这样表示：$O(f(n))$
            * 里面有一个函数f(n), 这个函数很简明，可以界定等号左边的函数, 它的波动趋势是由这个f(n)来描述的
            * 它会形成一个上界，可以有这样一个公式：$T(n) = O(f(n))$, 直观一些，可以把等号和Big-O去掉，即：
            * $T(n) \leq c \cdot f(n)$, 可以认为f(n)就是T(n)的一个上界
            * o与O的区别就在于这个等号，如果是Big-O，就是 $\leq$, 如果是small-o，就是 $<$
        * 这里f(n)是$o(n^{log_b a})$的意思是：$f(n) < o(n^{log_b a})$, 这时候算法的复杂度主要由更大的一项$o(n^{log_b a})$表示
        * 即：$T(n) = \Theta(n^{log_b a})$
        * 举个例子：$T(n) = 2 \cdot T(n/4) + O(1) = O(\sqrt{n})$
            * 在求解一个规模为n的问题的时候，把它拆分为2部分，每一部分的规模可以缩小到n/4而非n/2，为此只需要花O(1)的时间
            * 这时候我们有：$n^{log_4 2} = n^{\frac{1}{2}} = \sqrt{n}$
            * 典型的代表就是： kd-tree
    * 上图表格第二排是另一种情况，是严格的等于，它的标记是 $\Theta$
        * 这时候，$f(n) = \Theta(n^{c})$
        * 此时，$T(n) = \Theta(n^{log_b a}) \cdot log n$, 注意后面一定要乘上一个logn
        * 举个例子
            * $T(n) = 1 \cdot T(n/2) + O(1) = O(log n)$
            * 在二分查找的例子中，求解一个规模为n的问题，在n个元素中去搜索，我们会经过1次时间的比较, 舍弃左边或右边的一半
            * 把问题转换为规模为n/2的问题，这时候有，$log_2 1 = 0$，此时 $n^{log_2 1} = 1$, 此时，$T(n) = logn$
    * 上图表格第三排又是另一种情况，为了完成减而治之需要做的分与合的工作量非常大，以至于 $f(n) > n^{log_b a}$
        * 这时候，$f(n) = \omega(n^{log_b a})$，此时，$T(n) = \Theta(f(n))$
    * 综上所述，以上都是基于一种比较，大就留下来，小就离开，如果相等的时候，要再乘上一个logn

### Greatest Slice

1 ) **概述**

- 这是分而治之解决问题的一个例子，Greatest Slice
- slice 是统计学中的一个概念，有一个统计的对象叫做squence序列，如这个整数的序列，有正有负，可能还是零
- 所谓的slice就是切片，是这个序列中连续的一段，只要一个序列给出后，中间任何连续的一段都是一个slice
- 如果这个序列是由一系列的整数构成，我们就可以为这个序列每一个slice赋予一个整数的指标
- 这个指标就是这个slice所涵盖的整数项的总和，每一个slice中所涵盖的整数项的总和，我们就称为是这个slice的指标
- Greatest Slice 就是找到所有slice中总和最大的指标
- 举一个在商业中的例子：某一个序列可能是一家企业在一段时间经营的盈亏数据
    * 正的表示受益，负数表示亏空，0表示盈亏平衡
    * 需要知道在历史上哪段时间表现的是比较好的，在这个过程中受益达到最大
    * 这种数据可以帮助企业反思，为什么会有这种现象，后续应该怎么做
- 在Greatest Slice中可能会有一些歧义，比如在两个slice中表现相同，这时候我们更倾向于取更短的slice，即项数更少的，如下图

<div align="center">
    <img width="600" src="./screenshot/126.jpg">
    <br />
    <div style="text-align:center">备注：图片托管于github，请确保网络的可访问性</div>
    <br />
</div>

- 上图除了线条画出来的两个之外暗含第三个，也就是包含第二个之后的0的可能
- 可能有一个0缀在一个Greatest Slice的末尾，当然也可能在其前面
- 我们要求是最短的，这种0会被排除，在选取的时候，我们要盯住最大的指标，而且使序列尽可能的短
- 所以，尽管Greatest Slice可能有多个解，但其总和可能是一样的

2 ) **Brute Force**

- 下面是蛮力算法示例：
    ```cpp
    // 蛮力
    int gs_BF(int A[], int n) {
        int gs = A[0]; // 当前已知的最大和
        for(int i = 0; i < n; i++) {
            // 枚举所有的O(n^2)个区段
            for(int j = i; j < n; j++) {
                int s = 0;
                for(int k = i; k <= j; k++) s += A[k]; // 用O(n)的时间求和
                if(gs < s) gs = s; // 择优更新
            }
        }
        return gs;
    }
    ```
- 这个算法非常的蛮力，达到了$O(n^3)$的复杂度

<div align="center">
    <img width="600" src="./screenshot/127.jpg">
    <br />
    <div style="text-align:center">备注：图片托管于github，请确保网络的可访问性</div>
    <br />
</div>

- 如上图，如果绿色的为整体的序列，那么任何一个介于i和j之间的一段, 都是一个slice, 既然要找到最大的
- 最笨的方法是把每个slice都枚举一下，把所有介于i和j之间的slice都枚举出来，之后将每个slice的元素累加起来得到该slice的指标
- 最后将指标最大的slice找出来，这种方式可行，但效率太低, 实际中不能应用
- 我们一般不把这个解决方案叫做算法，写出来非常丢人，但是我们可以对其进行优化

2 ) **Incremental Stratey**

- 我们接着上面的蛮力计算，可以对内部的for循环进行优化
- 一开始算法是先枚举i, 每当枚举出固定的i之后, j就会从i开始去枚举所有以i为起点的slice, 长度为:1, 2, 3, ...
- 每当到j之后，无一例外将总和再加起来，这个显然效率非常低，其中间有很多工作可以不必做，这里存在一种递增的玩法
- 如下算法示例

```cpp
int gs_IC(int A[], int n) {
    // 当前已知的最大和
    int gs = A[0];
    // 枚举所有的起始于i终止于j的区间
    for(int i = 0; i < n; i++) {
        int s = 0;
        for(int j = i; j < n; j++) {
            s += A[k]; // 递增得到其总和：O(1)
            if(gs < s) gs = s; // 择优更新
        }
    }
    return gs;
}
```

- 也就是说，我要计算一连串的东西，这一连串的东西有相关性，也就是当我前一段算出来之后，我不必从头算下一个，而是直接累加即可
- 也就是直接做一次update即可，举个例子，从i~j-1和从i~j两个slice，当计算出i~j-1的slice之后，只需要再更新一次(做一次加法)即可得到i~j的slice
- 所以，在求所有slice总和的时候，并不需要每次都从头算起，只需要常数次时间O(1), 即可得到下一次总和，总和可增可减，我们只需保留最大的即可
- 整个算法，for循环关于i枚举，嵌套for循环关于j的枚举，只不过在每次i固定之后，它都会将s从内部for循环提升到外部来，s被反复使用，也就是一种incremental的方式
- 一开始s都是0，每一次加上新的单元，一个简单的加法操作代替了一个for循环操作，结果是一样的，但是效率更好
- 每次都将最大的slice得到，通过循环，不断得到所有的解，找到最大的slice
- 这个方法相较于之前有所改进，但是改进不大, 因为依然有两个循环

<div align="center">
    <img width="600" src="./screenshot/128.jpg">
    <br />
    <div style="text-align:center">备注：图片托管于github，请确保网络的可访问性</div>
    <br />
</div>

- 这里两个循环中，尽管每一次跑的长度是不断变化的，越来越短，但其形成的三角形面积也占了矩形的一半，如上图
- 其复杂度仍是O(n^2), 平方的算法其实都不是好的算法, 仍需要改进

3 ) **Divide-and-Conquer**

<div align="center">
    <img width="600" src="./screenshot/129.jpg">
    <br />
    <div style="text-align:center">备注：图片托管于github，请确保网络的可访问性</div>
    <br />
</div>

- 我们首先将这个问题一般化，求在lo~hi之间的这个Greatest Slice
- 一般分而治之，我们一般只需要平均分成两份即可：[lo, mi), [mi,hi)
    * 一般为：$A[lo, hi) = A[lo,mi) U A[mi,hi) = P U S$
- 每一子部分首先各自找到自己的Greatest Slice，之后找到横跨两边的Greatest Slice
    * 这里大概有三种可能：一、完全落在左边；二、完全落在右边；三、横跨左右两边
    * 借助递归，可求出P、S内部的GS
    * 第三种横跨也就是找以上图红线为终点的最大区间(深蓝部分：[i,mi))和红线为起点的最大区间(深绿部分:[mi,j))
        * 准确的说是覆盖A[mi,mi)的区段：$A[i,j)=A[i,mi)+A[mi,j)$
        * $S[i,mi) = max{S[k,mi) | lo \leq k < mi }$
        * $S[mi,j) = max{S[mi,k) | mi \leq k < hi }$
        * 两者可独立计算，累计耗时O(n)
    * 这时候我们用主定理来看下这个算法的复杂度：T(n) = 2 * T(n/2) + O(n) = O(n*logn)
    * 备注：通过比较$n^{log_2^2}$和n比较发现两者相等，之后乘上一个logn
    * 这时候，算法从之前的$n^2$降低到nlogn
- 关键代码
    ```cpp
    // Divide-And-Conquer: O(nlogn)
    int gs_DC(int A[], int lo, int hi) {
        if(hi - lo < 2) return A[lo];
        // 递归基在中点切分
        int mi = (lo + hi) / 2;
        // 枚举
        int gsL = A[mi-1], sL = 0, i = mi;
        // 所有[mi, j)类区段
        while(lo < i--) {
            if(gsL < (sL += A[i])) gsL = sL;
        }
        // 枚举
        int gsR = A[mi], sR = 0, j = mi - 1;
        // 所有[mi, j)类区段
        while(++j < hi) {
            // 更新
            if(gsR < (sR += A[j])) gsR = sR;
        }
        return max(gsL + gsR, max(gs_DC(A, lo, mi), gs_DC(A, mi, hi))); // 继续递归
    }
    ```
- 这个算法是否还可以继续优化呢?

4 ) **Decrease-and-Conquer**

<div align="center">
    <img width="600" src="./screenshot/130.jpg">
    <br />
    <div style="text-align:center">备注：图片托管于github，请确保网络的可访问性</div>
    <br />
</div>

- 我们还在这个[lo,hi)区间内，虽然一开始不知道GS具体在哪里，但是它总会存在于一段区间内: $GS(lo,hi) = A[i,j)$
- 然后，我们看下lo~hi之间的一些后缀(这里的后缀是指终点锁定在A[hi-1, hi)这一点上的slice)
- 我们要找总和非正的后缀A[k,hi), 即：S(k, hi) <= 0, S是函数不是区间，用()表示
- 假设后缀A[k,hi)这个slice的总和S在第一时间达到了不是正数的条件(<=0)，即有这样一个slice,我们称为slice1，它应该是最短的区间
- 假设这个slice1和我们GS存在交叉，那么交叉区间应该是：S(k,j)
- 我们知道S(k,j)所代表的区间A[k,j)是A[i,j)的后缀，而且这个区间的总和必须是正数
- 为什么这个总和必须为正数，如果非正，能舍弃，那原区间就构不成GS了就完全可以抹除这个后缀了
- 即：S(k,j) > 0，又 S(k,hi) <= 0, 同时 k < j, 通过递推，我们知道: S(j,hi) < 0 这个区间我们用slice2表示
- 这时候，S(j,hi)比S(k,hi)区间上比较还要短，即：slice2比slice1还要短，这时候我们知道这个和slice1是最短的总和为负的区间相矛盾了
- 所以，区间A[i,j)和区间A[k,hi)之间没有交集
- 我们通过反证得到：**最短的总和非正的后缀A[k,hi), 必然与GS(lo,hi)=A[i,j)无交**
- 这时候，我们可以通过**减而治之**的策略，通过一趟线性扫描在线性时间内找出GS
    * 从末尾开始遍历，查找后缀，当总和变成了非正，就可以把该区间割除，不会影响GS所在区间
    * 同时在每个被割除的区间可能存在GS,找到每个区间的GS,更新记录最大的GS
    * 重复不断的做这个事情，找到唯一那个GS，简单明了
- 这时候算法的复杂度为：O(n)

- 图示

<div align="center">
    <img width="600" src="./screenshot/131.jpg">
    <br />
    <div style="text-align:center">备注：图片托管于github，请确保网络的可访问性</div>
    <br />
</div>

- 示例代码
    ```cpp
    // Linear Scan: O(n)
    int gs_LS(int A[], int n) {
        int gs = A[0], s = 0, i = n, j = n;
        // 对于当前去加[i,j)
        while(0 < i--) {
            // 递归地得到其总和：O(1)
            s += A[i];
            if(gs < s) gs = s; // 择优，更新
            if(s <= 0) {s = 0; j = i;} // 剪除负和后缀
        }
        return gs;
    }
    ```

### Multiplication: Naive + DAC

<div align="center">
    <img width="600" src="./screenshot/132.jpg">
    <br />
    <div style="text-align:center">备注：图片托管于github，请确保网络的可访问性</div>
    <br />
</div>

- 这里聊下乘法中的分而治之，如上图
- 一般如果两个n位数相乘，得到的一个新数一定是2n位数字，比如：900*900=810000
    * 但也有例外：100*100=10000
    * 所以我们用一般性来表述，表示一个大约的估计
    * 我们可以把10000看做010000这样占位就说的通了
- 我们做数字的乘法的时候一般都是在稿纸上逐位相乘，现在有了复杂度的概念，我们知道这个算法的复杂度是O(n^2)
- 也就是说被乘数和乘数的每一个数位的组合都要做一次乘法，显然这个算法并不好，我们希望算的更快
- 我们想要用分而治之来优化这个问题，我们把这两个n位数分别拆解成n/2位数
- 这里绿色的n就会分解为n/2的绿色A和n/2的绿色B，同理蓝色的n也做同样分解，如上图
- 在计算的时候，我们把n/2的B与n/2的D相乘计算出一个n位的BD, 同理，对高位而言，得到一个AC
- 作为乘法还有交叉项，不论A和D相乘还是B和C相乘都是n位数，只不过对其的位置不同，也就是存在补位0(后面会有补充)，如上图它们的橙色位置
- 最后把计算出的四位数对齐加起来，得到最后的结果，我们来分析一下这个算法的复杂度
- $T(n) = 4 * T(n/2) + O(n) = O(n^{log_2 4}) = O(n^2)$
    * 这里分成了4个子任务，而且每个子任务的规模都是n/2, 为此只需要花O(n)的时间
    * 关于上面的O(n) 其实是: 4 * n/2 = 2n, 我们注意到AC、BC、AD后面都有补位0, 在计算机中我们用移位起来实现，所以常数时间即可解决问题
    * 所以最后的相加操作的时间复杂度是O(n)
    * 根据主定理，总体耗时：$O(n^2)$
- 显然，这个分治方法并没有优化，问题出在哪儿？我们先前分成了4个子任务，能否进行优化？
- 利用数学恒等式：$B*C + A*D = A*C + B*D - (A - B) * (C - D)$
- 我们知道乘法复杂度远远高于加和减, 上面表面上将2个乘法变成了3个乘法，感觉问题变得更糟糕了
- 但是基于上图我们知道，$A*C$和$B*D$都已经被计算过了，我们只需要关注$(A - B) * (C - D)$即可
- 所以实际上，我们把2个乘法变成了1个乘法，实际上是优化的
- 总体看来，一共有3个乘法，它们分别是:
    * $A*C$
    * $B*D$
    * $(A - B) * (C - D)$
- 这样，我们根据主定理来计算一下它的时间复杂度
- $T(n) = 3 * T(n/2) + O(n) = O(n^{log_2 3}) \approx O(n^{1.585})$
- 如下图示

<div align="center">
    <img width="600" src="./screenshot/133.jpg">
    <br />
    <div style="text-align:center">备注：图片托管于github，请确保网络的可访问性</div>
    <br />
</div>

### Mergesort: DAC

- 这里我们通过归并排序的例子来说明一下分而治之的策略

<div align="center">
    <img width="600" src="./screenshot/134.jpg">
    <br />
    <div style="text-align:center">备注：图片托管于github，请确保网络的可访问性</div>
    <br />
</div>

- 当我们面对一个足够长的序列进行排序的时候，我们一般会把问题从中间一分为二，前后两部分各自进行自己的任务
- 当子任务仍旧很大的时候，也就是说不方便排序的时候，继续一分为二，这样不断的一分为二的下去，直到分到足够小，最后剩下一个元素
- 一个元素其实是已经排好序的了，之后进行组合merge，如上图所示，在merge的过程中进行排序
- 由此可知，解决问题的实质过程并不在分的阶段，而在于合的阶段，而每一次merge其实拿到的都是两个有序的序列
- 每一次merge的过程是把两个有序的序列合并起来变成一个整体有序的序列
- 总结一下，Mergesort的过程就是不断自顶而下的去做分，一直分到每一个元素，然后再自底而上的去做归并
- 这个算法可以很简明的写出来，参考代码如下

    ```cpp
    template <typename T>
    // [lo, hi)
    void Vector<T>::mergeSort(Rank lo, Rank hi) {
        if(hi - lo < 2) return; // 单元素区间自然有序
        // 否则
        int mi = (lo + hi) >> 1; // 以中点为界
        mergeSort(lo, mi); // 对前半段排序
        mergeSort(mi, hi); // 对后半段排序
        merge(lo, mi, hi); // 归并
    }
    ```

- 算法图示如下

<div align="center">
    <img width="400" src="./screenshot/135.jpg">
    <br />
    <div style="text-align:center">备注：图片托管于github，请确保网络的可访问性</div>
    <br />
</div>

- 最终merge的技巧是Mergesort实质的技巧，用到了二路归并(2-Way Merge)的算法

**2-Way Merge**

<div align="center">
    <img width="600" src="./screenshot/136.jpg">
    <br />
    <div style="text-align:center">备注：图片托管于github，请确保网络的可访问性</div>
    <br />
</div>

- 有两个各自有序的序列，我们用红和蓝进行标识，假想它们就是两个队列
- 在任何时候可以忽略队伍后面的所有元素，只看队列中的第一个元素，如：5和2
- 然后将最小的一个元素取出来存入一个空的队列中，如图(b)
- 2被摘除之后，后面的所有元素会顺次的补上来，算法开始进行下一轮的迭代
- 同样的，将目光盯住队列的头部元素，这时候是5和4，取出最小的元素4存入之前的队列中，如图(c)
- 反复进行这类操作，将最小的摘出来存入队列
- 最终会变成一个整体有序的序列，这个过程只需要线性的时间就够了，也就是O(n)
- 由于核心的merge的过程只需要线性的时间，而Mergesort的时间复杂度我们可以计算一下
- T(n) = 2 * T(n/2) + O(n) = O(nlogn)
- 由此可见，Mergesort的时间复杂度为：O(nlogn)
- 其实，mergesort非常的有名，是历史上第一个达到O(nlogn)效率的算法
- 在大数据处理方面，内存不容易操作的时候是一个非常好的算法
- 这个算法背后的哲学依然是分而治之，准确来讲，就是把它分为两个规模相当的任务
- 而且诀窍是无论分还是合，都能够在颇为有效的这个算法以内完成

### 未完 待续