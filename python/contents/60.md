### Scrapy 命令

- 全局命令 (Anywhere on your computer) $ `scrapy -h` 或 $ `scrapy -help`

```shell
Johnny:~ johnny$ scrapy -h
Scrapy 1.8.0 - no active project

Usage:
  scrapy <command> [options] [args]

Available commands:
  bench         Run quick benchmark test
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

Use "scrapy <command> -h" to see more info about a command
```

- 项目命令 (Only in your project) $ `scrapy -h` 或 $ `scrapy -help`

```shell
Johnny:sp_pro1 johnny$ scrapy -h
Scrapy 1.8.0 - project: sp_pro1

Usage:
  scrapy <command> [options] [args]

Available commands:
  bench         Run quick benchmark test
  check         Check spider contracts
  crawl         Run a spider
  edit          Edit spider
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  list          List available spiders
  parse         Parse URL (using its spider) and print the results
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

Use "scrapy <command> -h" to see more info about a command

```

- 项目命令比全局命令要多

### Scrapy 命令具体使用
1 ）查看所有命令(帮助)

$ `scrapy -h`

2 ) 通过URL下载

示例： $ `scrapy fetch http://www.baidu.com` 用处不多

3 ) 获取配置信息

示例： $ `scrapy settings --get=CONCURRENT_ITEMS`

注：一般scrapy框架的配置源码会在python目录下的：`../../../Lib/site-packages/scrapy/settings/` 目录，这里是windows示例

4 ) 新建一个工程

$ `scrapy startproject your_project_name`

注：进入相关工程目录下运行的就是项目命令

5）构建爬虫genspider(generator spider), 一个工程中可以存在多个spider, 但是名字必须唯一

$ `scrapy genspider name domain` 示例：$`scrapy genspider taobao taobao.com`

6 )  查看当前项目内有多少爬虫

$ `scrapy list`

7 )  view命令下载并使用浏览器打开网页

$ `scrapy view http://www.baidu.com`

8 ）scrapy的命令交互

$ `scrapy shell http://www.baidu.com`

会有一个`response`对象可用于测试

示例：

```shell
scrapy shell http://www.baidu.com
...
...
>>> response.status
200
>>> response.url
'http://www.baidu.com'
>>> response.xpath("//title/text()")
[<Selector xpath='//title/text()' data='百度一下，你就知道'>]
```

### 常用scrapy命令：

1. 创建一个爬虫项目或工程 $ `scrapy startproject`
2. 在项目下创建一个爬虫spider类 $ `scrapy genspider`
3. 运行一个爬虫spider类文件 $ `scrapy runspider`
4. 查看当前项目下有多少个爬虫 $ `scrapy list`
5. 通过名称指定运行爬取信息 $ `scrapy crawl`
6. 使用shell进入scrapy交互环境 $ `scrapy shell`

### scrapy的应用实践之Get请求获取5i5j楼盘数据信息

1 ) 具体步骤

- ①、使用scrapy startproject命令来创建一个项目 `sp_pro1`

	项目目录

	```
	sp_pro1
	├── sp_pro1
	│   ├── __init__.py
	│   ├── __pycache__
	│   ├── items.py        # Items的定义，定义抓取的数据结构
	│   ├── middlewares.py  # 定义Spider和DownLoader的Middlewares中间件实现。 
	│   ├── pipelines.py    # 它定义Item Pipeline的实现，即定义数据管道
	│   ├── settings.py     # 它定义项目的全局配置
	│   └── spiders         # 其中包含一个个Spider的实现，每个Spider都有一个文件
	│       ├── __init__.py
	│       └── __pycache__
	└── scrapy.cfg    #Scrapy部署时的配置文件，定义了配置文件路径、部署相关信息等内容
	
	```

- ②、进入项目目录，创建爬虫spider类文件(执行genspider命令，第一个参数是Spider的名称，第二个参数是网站域名)

	执行命令：$ `scrapy genspider fang fang.5i5j.com`

	新的目录结构

	```
	├── sp_pro1
	│   ├── __init__.py
	│   ├── __pycache__
	│   │   ├── __init__.cpython-36.pyc
	│   │   └── settings.cpython-36.pyc
	│   ├── items.py
	│   ├── middlewares.py
	│   ├── pipelines.py
	│   ├── settings.py
	│   └── spiders
	│       ├── __init__.py
	│       ├── __pycache__
	│       │   └── __init__.cpython-36.pyc
	│       └── fang.py  #在spiders目录下有了一个爬虫类文件fang.py
	└── scrapy.cfg
	
	```

	生成的 `fang.py` 文件
	
	```python
	# -*- coding: utf-8 -*-
	import scrapy
	
	class FangSpider(scrapy.Spider):
	    name = 'fang'
	    allowed_domains = ['fang.5i5j.com']
	    start_urls = ['http://fang.5i5j.com/']
	
	    def parse(self, response):
	        pass
	```
	
	这里需要详细说明几点：
	
	①、Spider是自己定义的类，Scrapy用它来从网页中抓取内容，并解析抓取结果。
	②、此类继承Scrapy提供的Spider类scrapy.Spider，类中有三个属性：name、allowed_domains、start_urls和方法parse。
	③、`name`：是每个项目唯一名字，用于区分不同Spider。
	④、`allowed_domains`: 它是允许爬取的域名，如果初始或后续的请求链接不是这个域名，则请求链接会被过滤掉
	⑤、`start_urls`： 它包含了Spider在启动时爬取的URL列表，初始请求是由它来定义的。
	⑥、`parse` 方法： 调用start_urls链接请求下载执行后则调用parse方法，并将结果传入此方法。

- ③、进入items.py创建自己的Item容器类
    * Item是保存爬取数据的容器，它的使用方法和字典类型，但相比字典多了些保护机制。
    * 创建Item需要继承scrapy.Item类，并且定义类型为scrapy.Field的字段：（标题、开盘时间、单价）

	```python
	# 具体代码如下：
	import scrapy
	class FangItem(scrapy.Item):
	    # define the fields for your item here like:
	    title = scrapy.Field()
	    time = scrapy.Field()
	    price = scrapy.Field()
	```

- ④、进入自定义的spider类，解析Response信息，并封装到Item中
    * 在fang.py文件中，parse()方法的参数response是start_urls里面的链接爬取后的结果。
    * 提取的方式可以是CSS选择器、XPath选择器或者是re正则表达式。
	
	```python
	# 示例：
	# -*- coding: utf-8 -*-
	import scrapy
	from sp_pro1.items import FangItem
	
	class FangSpider(scrapy.Spider):
	    name = 'fang'
	    allowed_domains = ['fang.5i5j.com']
	    #start_urls = ['http://fang.5i5j.com/']
	    start_urls = ['https://fang.5i5j.com/bj/loupan/']
	
	    def parse(self, response):
	        hlist = response.css("div.houseList_list")
	        for vo in hlist:
	            item = FangItem()
	            item['title'] =  vo.css("h3.fontS20 a::text").extract_first()
	            item['time'] =  vo.re("<span>(.*?)开盘</span>")[0]
	            item['price'] =  vo.css("i.fontS24::text").extract_first()
	            #print(item)
	            yield item
	```

- ⑤、使用Item Pipeline项目管道对解析出来的Item数据进行清理、验证、去重、存储。
    * Item Pipeline为项目管道，当Item生产后，他会自动被送到Item Pipeline进行处理：
    * 我们常用Item Pipeline来做如下操作：
        * 清理HTML数据
        * 验证抓取数据，检查抓取字段
        * 查重并丢弃重复内容
        * 将爬取结果保存到数据库里
    * 执行爬取命令来进行爬取信息

- ⑥、运行
    * 执行如下命令来启用数据爬取 $ `scrapy crawl fang`
    * 将结果保存到文件中: 格式：json、csv、xml、pickle、marshal等
    
    ```shell
        scrapy crawl fang -o fangs.json
        scrapy crawl fang -o fangs.csv
        scrapy crawl fang -o fangs.xml
        scrapy crawl fang -o fangs.pickle
        scrapy crawl fang -o fangs.marshal
    ```

2 ）注意事项

- ①、如果项目运行不起来，建议根据错误信息进行项目配置，常见的处理方式是

	在settings中添加或开启

	```config
	HTTPERROR_ALLOWED_CODES = [403]
	USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'
	ROBOTSTXT_OBEY = False
	ITEM_PIPELINES = {
	    'sp_pro1.pipelines.SpPro1Pipeline': 300,
	}
	```

- ②、以上处理形式基于当时的技术，存在时效性，仅供学习参考

### scrapy的应用实践之Post请求处理

- 在Scrapy框架中默认都是GET的提交方式，但是我们可以使用FormRequest来完成POST提交，并可以携带参数。

- 如下案例为有道词典的翻译信息爬取案例，网址：http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule

- 首先创建一个youdao有道的爬虫文件：$ `scrapy genspider youdao fanyi.youdao.com`

- 编写爬虫文件，注意返回的是json格式，具体代码如下：

```python
# -*- coding: utf-8 -*-
import scrapy,json

class YoudaoSpider(scrapy.Spider):
    name = 'youdao'
    allowed_domains = ['fanyi.youdao.com']
    #start_urls = ['http://fanyi.youdao.com']

    def start_requests(self):
        url = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'
        keyword = input("请输入要翻译的单词：")
        data = {'i':keyword,'doctype': 'json',}
        # FormRequest 是Scrapy发送POST请求的方法
        yield scrapy.FormRequest(
            url = url,
            formdata = data,
            callback = self.parse
        )

    def parse(self, response):
        res = json.loads(response.body) # 处理json数据
        print(res['translateResult'][0][0]['tgt'])
        input("按任意键继续")
```

注意事项：

如果不写`start_urls`属性, 那么`start_requests`方法会自动被调用, 所以上文注释了`start_urls`