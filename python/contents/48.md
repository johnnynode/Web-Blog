### 概述

网络爬虫(web spider) 又称为网络蜘蛛、网络机器人，是一种按一定规则，自动抓取万维网信息的程序或脚本

### 按系统结构和实现技术进行分类

- 通用网络爬虫：尽可能大的网络覆盖率，如百度，谷歌搜索

- 聚焦网络爬虫：有目标性，选择性访问万维网爬取信息

- 增量式网络爬虫：只爬取新产生或已经更新的网页信息。特点：耗费少，难度大

- 深层网络爬虫：通过提交一些关键字才能获取的Web页面, 如登录或注册后访问的页面

在实际应用中通常是以上几种爬虫技术的结合实现

### 爬虫应用场景

- 科学研究：在市场上通过爬虫获取大量数据，获取我们所需要的信息，进行科学研究

- Web安全：通过爬虫实现漏洞检测功能

- 产品研发：通过获取的数据，进行分析，进行市场研究，可以更好的研发新产品

- 舆情监控：分析识别如微博数据中某些用户是否是水军

### 网络爬虫的合法性

- 在很多网站根路径下会有个`robots.txt`文档，如果没有这个文档，那么网站所有数据都可以爬取
- 在有次文档的网站下，需要判断是否有禁止访客获取数据
- 文档举例: `https://www.taobao.com/robots.txt` 可以查看淘宝网站里面的具体规则

### 网络爬虫的执行过程

- ①. 获取初始URL
- ②. 爬取存储页面内容, 并获取新的URL
- ③. 将新的URL放在存储队列中
- ④. 在存储队列中读取新的URL
- ⑤. 判断是否满足结束条件，如果是，则停止爬取；如果否，则回到②

### 单项爬取中的几个工具

通过上面可知道如何批量执行，下面说下单项的执行过程

- ①. 主调度器：用于控制调度整个爬取过程
- ②. url管理器：使用url管理器获取url
- ③. 下载器：下载url中的内容
- ④. 解析器：解析url中的数据
- ⑤. 数据库：对有价值的数据进行更新、入库

### 网络爬虫使用的技术

- 网络爬虫框架：scrapy
- python中相关的库：`urllib`、`urllib3`、`requests`、`mechanize`、`selenium`、`splinter`
    * 其中 `urllib`、`urllib3`、`requests`、`mechanize` 用来获取URL对应的原始响应内容 (高效)
    * 其中 `selenium`、`splinter` 通过加载浏览器驱动, 获取浏览器渲染后的响应内容，模拟程度更高 (低效)
- 对于爬取的过程，主要是模拟浏览器向服务器发送构造好的http请求，常见类型有：`get` / `post`
- 对于数据解析方面，有相应的库：`lxml`, `beautifulsoup4`, `re`, `pyquery`等，常用方法：`xpath路径表达式`、`css选择器` 、`正则表达式`等
    * `xpath路径表达式` 、 `css选择器` 主要用于提取结构化数据
    * `正则表达式` 用于提取非结构化的数据

### 爬虫其他相关技术：

- 数据抓取
    * HTTP 协议、身份认证机制(cookie)
    * 网络流量分析: Chrome、Firefox,Firebug、Fiddler、Burpsuit

- 数据解析
    * HTML结构、JSON数据格式、XML数据格式
    * CSS选择器、Xpath路径表达式、正则表达式、Python编码/解码

- 数据入库
    * 结构化数据库：MySQL、SQLite
    * 非结构化数据库：Redis

- 其他
    * 多线程、任务调度、消息队列、分布式爬虫、图像识别、反爬虫技术
