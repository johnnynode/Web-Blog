### 常见的反爬虫策略和反反爬策略

一般网站从三个方面反爬虫, 前两种比较容易遇到，大多数网站都从这些角度来反爬虫。第三种一些应用ajax的网站会采用，还有一些是数据推送，这样增大了爬取的难度:

1) 通过Headers反爬虫

- 从用户请求的Headers反爬虫是最常见的反爬虫策略。
- 很多网站都会对Headers的User-Agent进行检测
- 有一部分网站会对Referer进行检测（一些资源网站的防盗链就是检测Referer）。

针对这类网站如何反反爬？

- 对于检测Headers的反爬虫，在爬虫中修改或者添加Headers就能很好的绕过。
- 将浏览器的User-Agent复制到爬虫的Headers中；或者将Referer值修改为目标网站域名。

2）基于用户行为反爬虫

- 通过检测用户行为例如同一IP短时间内多次访问同一页面
- 通过检测用户行为或者同一账户短时间内多次进行相同操作

针对这类网站如何反反爬?

- 大多数网站都是前一种情况，对于这种情况，使用IP代理就可以解决。
- 对于第二种情况，可以在每次请求后随机间隔几秒再进行下一次请求。
- 有些有逻辑漏洞的网站，可以通过请求几次，退出登录，重新登录，继续请求来绕过同一账号短时间内不能多次进行相同请求的限制。

3) 动态页面的反爬虫

- 大多网站界面都是静态页面(即在浏览器中查看源代码可见)
- 有一部分网站中的数据是后期通过ajax请求(或其他方式如推送技术)得到。

针对这类网站如何反反爬?

- 解决办法：首先用Firebug或者Fiddler对网络请求进行分析, 找到ajax的请求url, 通过Python模拟请求得到需要的数据。
- 但是还有些网站把ajax请求的所有参数全部加密了，针对于这方式我们后面会给大家讲解动态渲染页面信息爬取。

### 关于请求头Headers相关说明

这部分总共分为：请求和响应两大部分

1）请求：(客户端->服务端[request]) 

```log
GET(请求的方式) /newcoder/hello.html(请求的目标资源) HTTP/1.1(请求采用的协议和版本号) 
Accept: */*(客户端能接收的资源类型) 
Accept-Language: en-us(客户端接收的语言类型) 
Connection: Keep-Alive(维护客户端和服务端的连接关系) 
Host: localhost:8080(连接的目标主机和端口号) 
Referer: http://localhost/links.asp(告诉服务器我来自于哪里) 
User-Agent: Mozilla/4.0(客户端版本号的名字) 
Accept-Encoding: gzip, deflate(客户端能接收的压缩数据的类型) 
If-Modified-Since: Tue, 11 Jul 2000 18:23:51 GMT(缓存时间)  
Cookie(客户端暂存服务端的信息) 
Date: Tue, 1 Jul 2020 08:08:08 GMT(客户端请求服务端的时间)
```

2) 响应：(服务端->客户端[response])

```log
HTTP/1.1(响应采用的协议和版本号) 200(状态码) OK(描述信息)
Location: http://www.baidu.com(服务端需要客户端访问的页面路径) 
Server:apache tomcat(服务端的Web服务端名)
Content-Encoding: gzip(服务端能够发送压缩编码类型) 
Content-Length: 80(服务端发送的压缩数据的长度) 
Content-Language: zh-cn(服务端发送的语言类型) 
Content-Type: text/html; charset=GB2312(服务端发送的类型及采用的编码方式)
Last-Modified: Tue, 11 Jul 2000 18:23:51 GMT(服务端对该资源最后修改的时间)
Refresh: 1;url=http://www.baidu.com(服务端要求客户端1秒钟后，刷新，然后访问指定的页面路径)
Content-Disposition: attachment; filename=aaa.zip(服务端要求客户端以下载文件的方式打开该文件)
Transfer-Encoding: chunked(分块传递数据到客户端）  
Set-Cookie:SS=Q0=5Lb_nQ; path=/search(服务端发送到客户端的暂存数据)
Expires: -1//3种(服务端禁止客户端缓存页面数据)
Cache-Control: no-cache(服务端禁止客户端缓存页面数据)  
Pragma: no-cache(服务端禁止客户端缓存页面数据)   
Connection: close(1.0)/(1.1)Keep-Alive(维护客户端和服务端的连接关系)  
Date: Tue, 1 Jul 2020 08:08:08 GMT(服务端响应客户端的时间)
```

### 在请求头Headers中的伪装和处理

1) 在requests中设置请求头Headers

```python
import requests

# 代理IP地址
proxy = {'HTTP':'117.85.105.170:808','HTTPS':'117.85.105.170:808'}
# header头信息
headers = {
    'Host': 'blog.csdn.net',
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:43.0) Gecko/20100101 Firefox/43.0',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
    'Accept-Encoding': 'gzip, deflate',
    'Referer': 'http://www.baidu.com',
    'Connection': 'keep-alive',
    'Cache-Control': 'max-age=0',
}
# 请求url地址
url = "http://blog.csdn.net"

# 提交请求爬取信息
response = requests.get(url,headers=headers,proxies=proxy)

# 获取响应码
print(response.status_code)
```

2) 浏览器用户代理池信息用于随机选择头信息

```python
import random
user_agents = [
    'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',
    'Opera/9.25 (Windows NT 5.1; U; en)',
    'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)',
    'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)',
    'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12',
    'Lynx/2.8.5rel.1 libwww-FM/2.14 SSL-MM/1.4.1 GNUTLS/1.2.9',
    'Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7',
    'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0',
]
agent = random.choice(user_agents) # 随机获取一个浏览器用户信息
```

### Ajax信息的爬取

有些数据并非服务端渲染，而是通过后来加载的数据，我们可以分析页面数据，进行爬取处理，下面我们来通过jd评论信息的爬取来实践

相关示例代码如下：

```python
import requests
import re, json

# header头信息
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:43.0) Gecko/20100101 Firefox/43.0',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
    'Accept-Encoding': 'gzip, deflate',
    'Referer': 'https://item.jd.com/100005171461.html',
    'Connection': 'keep-alive',
    'Cache-Control': 'max-age=0',
}

#请求url地址, 此处实际上是一个jsonp请求
url = "https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv1950&productId=100005171461&score=0&sortType=5&page=0&pageSize=10&isShadowSku=0&fold=1"

# 提交请求爬取信息
response = requests.get(url,headers=headers)
txt = response.text

start = txt.find('(') + 1
end = txt.rfind(");")

jsonstr = txt[start:end]
data_json = json.loads(jsonstr)
comments = data_json.get('comments')

for c in comments:
    print(c.get('content'))
    print("="*80)
```