### 关于Selector选择器

- 对用爬取信息的解析，我们在之前已经介绍了正则re、Xpath、Beautiful Soup和PyQuery。
- 而Scrapy还给我们提供自己的数据解析方法，即Selector（选择器）。
- Selector（选择器）是基于lxml来构建的，支持XPath、CSS选择器以及正则表达式，功能全面，解析速度和准确度非常高。
- Selector（选择器）是一个可以独立使用的模块。直接导入模块就可以实例化使用。
- 可以使用Scrapy shell 来模拟请求实现命令行交互模式

### Selector选择器的使用

1）直接使用

字符串模拟案例：

```python
from scrapy import Selector

content="<html><head><title>My html</title><body><h3>Hello Word!</h3></body></head></html>"
selector = Selector(text=content)
print(selector.xpath('/html/head/title/text()').extract_first())
print(selector.css('h3::text').extract_first())
```

2）Scrapy shell 与 Xpath选择器的使用

借助于Scrapy shell来模拟请求的过程，然后把一些可操作的变量传递给我们，如request、response等。

百度首页案例：

```shell
johnny$ scrapy shell http://www.baidu.com
...
...
...
# 此处上面省略很多框架输出
# 注意：如果在 response.url 中显示 scrapy AttributeError: 'NoneType' object has no attribute 'url'
# 重新打开一个shell命令窗口(或命令提示符)即可解决
>>> response.url
'http://www.baidu.com'
>>> response.status
200
>>> response.selector.xpath("/html/head/title/text()")
[<Selector xpath='/html/head/title/text()' data='百度一下，你就知道'>]
>>> response.selector.xpath("/html/head/title/text()").extract()
['百度一下，你就知道']
>>> response.selector.xpath("/html/head/title/text()").extract_first()
'百度一下，你就知道'
>>> response.selector.xpath("/html/head/title/text()").extract()[0]
'百度一下，你就知道'
>>> response.selector.xpath("//a/text()").extract_first()
'新闻'
>>> response.selector.xpath("//a/@href").extract_first()
'http://news.baidu.com'

```

3）Xpath选择器补充：

3.1 response.selector属性返回内容相当于response的body构造了一个Selector对象。
3.2 Selector对象可以调用xpath（）方法实现信息的解析提取。
3.3 在xpath（）后使用extract（）可以返回所有的元素结果。
3.4 若xpath（）有问题，那么extract（）会返回一个空列表。
3.5 在xpath（）后使用extract_first（）可以返回第一个元素结果。

使用scrapy shell 爬取"淘宝网"->"商品分类"->"特色市场"的案例。

```shell
johnny$ scrapy shell https://www.taobao.com/tbhome/page/special-markets
...
...
...
# 此处上面省略很多框架输出
>>> response.url
'https://www.taobao.com/tbhome/page/special-markets'
>>> response.status
200
>>> response.selector.xpath("//dt/text()")
[<Selector xpath='//dt/text()' data='时尚爆料王'>, <Selector xpath='//dt/text()' data='品质生活家'>, <Selector xpath='//dt/text()' data='特色玩味控'>, <Selector xpath='//dt/text()' data='实惠专业户'>]
>>> dllist = response.selector.xpath("//dl[@class='market-list']")
>>> for v in dllist:
...     print(v.xpath("./dt/text()").extract_first())
... 
时尚爆料王
品质生活家
特色玩味控
实惠专业户
>>> for v in dllist:
...     print(v.xpath("./dt/text()").extract_first())
...     print("="*50)
...     alist = v.xpath(".//a")
...     for a in alist:
...         print(a.xpath("./@href").extract_first(), end=":")
...         print(a.xpath("./span/img/@alt").extract_first())
... 
时尚爆料王
==================================================
https://if.taobao.com/:潮流从这里开始
https://guang.taobao.com/:外貌协会の逛街指南
https://mei.taobao.com/:妆 出你的腔调
https://g.taobao.com/:探索全球美好生活
//star.taobao.com/:全球明星在这里
https://mm.taobao.com/:美女红人集中地
https://www.taobao.com/markets/designer/stylish:全球创意设计师平台
品质生活家
==================================================
https://chi.taobao.com/chi/:食尚全球 地道中国
//q.taobao.com:懂得好生活
https://www.jiyoujia.com/:过我想要的生活
https://www.taobao.com/markets/sph/sph/sy:尖货奢品品味选择
https://www.taobao.com/markets/qbb/index:享受育儿生活新方式
//car.taobao.com/:买车省钱，用车省心
//sport.taobao.com/:爱上运动每一天
//zj.taobao.com:匠心所在  物有所值
//wt.taobao.com/:畅享优质通信生活
https://www.alitrip.com/:比梦想走更远
特色玩味控
==================================================
https://china.taobao.com:地道才够味！
https://www.taobao.com/markets/3c/tbdc:为你开启潮流新生活
https://acg.taobao.com/:ACGN  好玩好看
https://izhongchou.taobao.com/index.htm:认真对待每一个梦想。
//enjoy.taobao.com/:园艺宠物爱好者集中营
https://sf.taobao.com/:法院处置资产，0佣金捡漏
https://zc-paimai.taobao.com/:超值资产，投资首选
https://paimai.taobao.com/:想淘宝上拍卖
//xue.taobao.com/:给你未来的学习体验
//2.taobao.com:让你的闲置游起来
https://ny.taobao.com/:价格实惠品类齐全
实惠专业户
==================================================
//tejia.taobao.com/:优质好货 特价专区
https://qing.taobao.com/:品牌尾货365天最低价
https://ju.taobao.com/jusp/other/mingpin/tp.htm:奢侈品团购第一站
https://ju.taobao.com/jusp/other/juliangfan/tp.htm?spm=608.5847457.102202.5.jO4uZI:重新定义家庭生活方式
https://qiang.taobao.com/:抢到就是赚到！
https://ju.taobao.com/jusp/nv/fcdppc/tp.htm:大牌正品 底价特惠
https://ju.taobao.com/jusp/shh/life/tp.htm?:惠聚身边精选好货
https://ju.taobao.com/jusp/sp/global/tp.htm?spm=0.0.0.0.biIDGB:10点上新 全球底价
https://try.taobao.com/index.htm:总有新奇等你发现
>>> 
```

4）CSS选择器

Selector对象可以调用css()方法实现信息的解析提取。

4.1 在`css()` 后使用`extract()`可以返回所有的元素结果。
4.2 在`css()` 有问题，那么`extract()`会返回一个空列表。
4.3 在`css()` 后使用`extract_first()`可返回第一个元素结果。

同xpath()一样，下面使用scrapy shell 爬取"淘宝网"->"商品分类"->"主题市场"的案例

```shell
johnny$ scrapy shell https://www.taobao.com/tbhome/page/market-list
...
...
...
# 此处上面省略很多框架输出
>>> response.url
'https://www.taobao.com/tbhome/page/market-list'
>>> response.status
200
>>> response.css("a.category-name-level1::text").extract()
['女装男装', '鞋类箱包', '母婴用品', '护肤彩妆', '汇吃美食', '珠宝配饰', '家装建材', '家居家纺', '百货市场', '汽车·用品', '手机数码', '家电办公', '更多服务', '生活服务', '运动户外', '花鸟文娱', '农资采购']
#获取淘宝页面中所有分类信息
>>> dlist = response.css("div.home-category-list")
>>> for dd in dlist:
...     print(dd.css("a.category-name-level1::text").extract_first())
...     print("="*50)
...     alist = dd.css("li.category-list-item")
...     for v in alist:
...         print(v.xpath("./a/text()").extract_first())
...         print("-"*50)
...         talist = v.css("div.category-items a")
...         for a in talist:
...             print(a.css("::text").extract_first(),end=" ")
...         print()
... 
女装男装
==================================================
潮流女装
--------------------------------------------------
羽绒服 毛呢大衣 毛衣 冬季外套 新品 裤子 连衣裙 腔调 
时尚男装
--------------------------------------------------
秋冬新品 淘特莱斯 淘先生 拾货 秋冬外套 时尚套装 潮牌 爸爸装 
性感内衣
--------------------------------------------------
春新品 性感诱惑 甜美清新 简约优雅 奢华高贵 运动风 塑身 基础内衣 
羽绒服
--------------------------------------------------
轻薄款 长款 短款 毛领 加厚 被子 鹅绒 新品 
秋外套
--------------------------------------------------
秋款 夹克 卫衣 西装 风衣 皮衣 毛呢外套 薄羽绒 
文胸
--------------------------------------------------
无钢圈 无痕文胸 蕾丝内衣 运动文胸 聚拢文胸 大码文胸 抹胸式 隐形 
呢外套
--------------------------------------------------
廓形 双面呢 羊绒 中长款 短款 毛领 设计师款 系带 
衬衫/T恤
--------------------------------------------------
T恤 长袖T 打底衫 纯色 衬衫 长袖款 商务款 时尚款 
家居服
--------------------------------------------------
睡衣套装 睡裙 睡袍浴袍 外穿家居 女士睡衣 男士睡衣 情侣睡衣 亲子睡衣 
毛衣
--------------------------------------------------
马海毛 貂绒 羊绒 羊毛 开衫 中长款 短款 卡通 
男士裤子
--------------------------------------------------
休闲裤 工装裤 运动裤 长裤 牛仔裤 小脚裤 哈伦裤 直筒裤 
内裤
--------------------------------------------------
女士内裤 男士内裤 三角裤 平角裤 丁字裤 阿罗裤 星期裤 低腰 
外套上衣
--------------------------------------------------
外套 套装 风衣 卫衣 真皮皮衣 马甲 小西装 唐装 中老年 
针织毛衫
--------------------------------------------------
薄毛衣 针织开衫 圆领毛衣 V领毛衣 纯色毛衣 民族风 羊毛衫 羊绒衫 
丝袜
--------------------------------------------------
船袜 男人袜 连裤袜 隐形袜 收腹裤 塑身衣 美体裤 收腹带 
# 此处会类似上面输出很多信息，此处省略一万字 ...
>>> 

```

特别注意： css中获取属性的方式：`a.css("::attr(href)").extract_first()`

5 ） 正则匹配：

还是用上面的shell测试：

```shell
>>> response.xpath("//head").re("<title>(.*?)</title>")
['淘宝首页行业市场']
>>> response.selector.re("<a .*?>(.*?)</a>")
['已买到的宝贝', '我的足迹', ...] # 此处输出很多，不列举了
>>> response.selector.re("<a .*?>(.*?)</a>")[0]
'已买到的宝贝'
>>> response.selector.re_first("<a .*?>(.*?)</a>")
'已买到的宝贝'
```